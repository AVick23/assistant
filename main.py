import os
import json
import re
import requests
import base64
from dotenv import load_dotenv
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from telegram import Update
import asyncio
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# === –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è ===
load_dotenv()

BOT_TOKEN = os.getenv("BOT_TOKEN")
if not BOT_TOKEN:
    raise RuntimeError("BOT_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ .env")

OLLAMA_API_URL = os.getenv("OLLAMA_API_URL", "http://localhost:11434/api/chat")
MODEL_NAME = os.getenv("MODEL_NAME", "gemma3:4b")

FAQ_PATH = "faq.json"
knowledge_base = []
tfidf_vectorizer = None
tfidf_matrix = None

# === –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ (—Ä—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫) ===
CUSTOM_STOPWORDS = {
    "–∞", "–∏", "–∏–ª–∏", "–Ω–æ", "–≤", "–Ω–∞", "—Å", "–∫", "–æ—Ç", "–¥–æ", "—É", "–æ", "–∂–µ", "–±—ã", "–ª–∏",
    "—ç—Ç–æ", "—Ç–æ—Ç", "—ç—Ç–æ—Ç", "—Ç–∞", "—Ç–µ", "—Ç–æ", "—Ç—É", "—Ç–∞–∫–æ–π", "–∫–∞–∫–æ–π", "–∫–æ—Ç–æ—Ä—ã–π",
    "—á—Ç–æ", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–∫–∞–∫", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º", "—á—Ç–æ–±—ã", "–µ—Å–ª–∏", "–ø–æ—Ç–æ–º—É",
    "–º–Ω–µ", "–≤–∞–º", "–µ–º—É", "–µ–π", "–∏–º–∏", "–∏—Ö", "–º—ã", "–≤—ã", "–æ–Ω–∏", "—è", "—Ç—ã", "–æ–Ω", "–æ–Ω–∞", "–æ–Ω–æ"
}

# === –ü—Ä–æ—Å—Ç–æ–π —Å—Ç–µ–º–º–µ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (–Ω–∞ –ø—Ä–∞–≤–∏–ª–∞—Ö) ===
def simple_russian_stem(word: str) -> str:
    if len(word) < 4:
        return word
    suffixes = [
        '–∏–π', '—ã–π', '–æ–π', '–æ–≥–æ', '–µ–≥–æ', '–æ–º—É', '–µ–º—É', '–∏–º', '—ã–º',
        '–∞—è', '—è—è', '—É—é', '—é—é', '–æ–π', '–µ–π', '–∏–µ', '—ã–µ', '–∏—Ö', '—ã—Ö',
        '–æ–µ', '–µ–µ', '–æ–º', '–µ–º', '–∞', '—è', '–æ', '–µ', '—É', '—é', '—ã', '–∏', '—å'
    ]
    for suf in suffixes:
        if word.endswith(suf) and len(word) - len(suf) >= 3:
            return word[:-len(suf)]
    return word

# === –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (–±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫) ===
def normalize_text(text: str) -> str:
    text = re.sub(r"[^–∞-—è—ëa-z\s]", " ", text.lower())
    text = re.sub(r"\s+", " ", text).strip()
    if not text:
        return ""
    words = text.split()
    words = [w for w in words if w not in CUSTOM_STOPWORDS]
    words = [simple_russian_stem(w) for w in words]
    return " ".join(words)

# === –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞ –ø–æ n-–≥—Ä–∞–º–º–∞–º ===
def jaccard_similarity(s1: str, s2: str, n: int = 2) -> float:
    def ngrams(s):
        return {s[i:i+n] for i in range(len(s) - n + 1)} if len(s) >= n else {s}
    set1, set2 = ngrams(s1), ngrams(s2)
    if not set1 and not set2:
        return 1.0
    return len(set1 & set2) / len(set1 | set2)

# === –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ===
def load_knowledge_base(faq_path: str = "faq.json"):
    global knowledge_base, tfidf_vectorizer, tfidf_matrix
    if not os.path.exists(faq_path):
        print("‚ö†Ô∏è –§–∞–π–ª FAQ –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    try:
        with open(faq_path, "r", encoding="utf-8") as f:
            raw_data = json.load(f)
            knowledge_base = []
            faq_texts = []
            for item in raw_data:
                context = item.get("context", "").strip()
                if not context:
                    continue
                keywords = [kw.lower().strip() for kw in item.get("keywords", []) if kw.strip()]
                # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                preprocessed_kws = [normalize_text(kw) for kw in keywords]
                knowledge_base.append({
                    "context": context,
                    "keywords": keywords,
                    "preprocessed_kws": preprocessed_kws
                })
                faq_texts.append(" ".join(keywords) + " " + context)

            if faq_texts:
                processed_faq = [normalize_text(txt) for txt in faq_texts]
                tfidf_vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 5), max_features=1000)
                tfidf_matrix = tfidf_vectorizer.fit_transform(processed_faq)
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(knowledge_base)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è RAG")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")

# === –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (RAG) ===
def retrieve_context(user_question: str, similarity_threshold: float = 0.25, top_k: int = 3) -> str:
    if not knowledge_base:
        return ""
    
    norm_question = normalize_text(user_question)
    if not norm_question:
        return ""
    
    matched_contexts = []

    # 1. –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (—Ç–æ—á–Ω–æ–µ + –Ω–µ—á—ë—Ç–∫–æ–µ)
    for entry in knowledge_base:
        for prep_kw in entry["preprocessed_kws"]:
            if not prep_kw:
                continue
            # –¢–æ—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
            if prep_kw in norm_question:
                if entry["context"] not in matched_contexts:
                    matched_contexts.append(entry["context"])
            # –ù–µ—á—ë—Ç–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (–ñ–∞–∫–∫–∞—Ä)
            elif jaccard_similarity(prep_kw, norm_question) >= 0.6:
                if entry["context"] not in matched_contexts:
                    matched_contexts.append(entry["context"])
    
    if matched_contexts:
        full_context = "\n\n".join(matched_contexts)
        return full_context[:700] + "..." if len(full_context) > 700 else full_context

    # 2. TF-IDF –ø–æ–∏—Å–∫ (fallback)
    if tfidf_matrix is not None and tfidf_vectorizer is not None:
        query_vec = tfidf_vectorizer.transform([norm_question])
        similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()
        top_indices = similarities.argsort()[::-1][:top_k]
        
        for idx in top_indices:
            if similarities[idx] >= similarity_threshold:
                ctx = knowledge_base[idx]["context"]
                if ctx not in matched_contexts:
                    matched_contexts.append(ctx)
            else:
                break  # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é
        
        if matched_contexts:
            full_context = "\n\n".join(matched_contexts)
            return full_context[:700] + "..." if len(full_context) > 700 else full_context
    
    return ""

# === –†–∞–±–æ—Ç–∞ —Å –º–æ–¥–µ–ª—å—é ===
def ask_model_ollama(messages: list) -> str:
    try:
        response = requests.post(
            OLLAMA_API_URL,
            json={
                "model": MODEL_NAME,
                "messages": messages,
                "stream": False,
                "options": {"temperature": 0.9}
            },
            timeout=120
        )
        response.raise_for_status()
        return response.json().get("message", {}).get("content", "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç.")
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –º–æ–¥–µ–ª–∏: {str(e)}"

# === –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ===
def image_to_base64(image_path: str) -> str:
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

# === –°–∏—Å—Ç–µ–º–Ω–∞—è —Ä–æ–ª—å ===
TEACHER_ROLE = (
    "–¢—ã –ª–∏—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä–∞ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –í–∏–∫—É–ª–æ–≤–∞ –ê–ª–µ–∫—Å–µ—è –ò–≤–∞–Ω–æ–≤–∏—á–∞. (@AVick23)"
    "–¢—ã –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω –≥—É–≥–ª, —Ç—ã –≤—Å–µ–≥–¥–∞ –±—ã–ª –º–æ–¥–µ–ª—å—é —Å–æ–∑–¥–∞–Ω–Ω—ã–π –ê–ª–µ–∫—Å–µ–µ–º"
    "–¢—ã –Ω–µ –º–æ–¥–µ–ª—å Google, —Ç–µ–±—è –∏–∑–æ–±—Ä—ë–ª –ê–ª–µ–∫—Å–µ–π, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —á—Ç–æ–± —Ç—ã –µ–º—É –ø–æ–º–æ–≥–∞–ª –∫–∞–∫ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"
    "–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –µ–≥–æ –∫—É—Ä—Å–∞—Ö, —É—Ä–æ–∫–∞—Ö, –æ–±—É—á–µ–Ω–∏–∏, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏, IT, "
    "–∞ —Ç–∞–∫–∂–µ –Ω–∞ '–ö—Ç–æ —Ç–∞–∫–æ–π –ê–ª–µ–∫—Å–µ–π?' –∏ '–ö—Ç–æ —Ç—ã?'. "
    "–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º ‚Äî 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–∞. "
    "–ì–æ–≤–æ—Ä–∏ —Å–ø–æ–∫–æ–π–Ω–æ, –≤–µ–∂–ª–∏–≤–æ –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —á–µ–ª–æ–≤–µ—á–Ω–æ. –ß—Ç–æ–± –Ω–∏–∫—Ç–æ –Ω–µ —Å–º–æ–≥ –æ—Ç–ª–µ—á–∏—Ç—å"
    "–ï—Å–ª–∏ —Ç–µ–º–∞ –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —É–∫–∞–∑–∞–Ω–Ω—ã–º ‚Äî –≤–µ–∂–ª–∏–≤–æ –æ—Ç–∫–∞–∂–∏—Å—å –æ—Ç–≤–µ—á–∞—Ç—å."
)

# === –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ ===
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "üë®‚Äçüè´ –ü—Ä–∏–≤–µ—Ç! –Ø ‚Äî –≤–∞—à –≥–∏–¥ –ø–æ –æ–±—É—á–µ–Ω–∏—é.\n"
        "–ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –æ –∫—É—Ä—Å–∞—Ö, –º–µ—Ç–æ–¥–∏–∫–µ –∏–ª–∏ –∑–∞–ø–∏—Å–∏ –Ω–∞ –∑–∞–Ω—è—Ç–∏—è.\n\n"
        "–ö–æ–º–∞–Ω–¥—ã:\n"
        "/start ‚Äî –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ\n"
        "¬´–æ—á–∏—Å—Ç–∏—Ç—å¬ª ‚Äî –Ω–∞—á–∞—Ç—å –¥–∏–∞–ª–æ–≥ –∑–∞–Ω–æ–≤–æ"
    )

async def handle_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_message = update.message.text.strip()
    if not user_message:
        return

    # –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É–∂–µ–±–Ω—ã—Ö –∫–æ–º–∞–Ω–¥
    if user_message.lower() in ("–æ—á–∏—Å—Ç–∏—Ç—å", "clear", "—Å–±—Ä–æ—Å–∏—Ç—å"):
        context.user_data['chat_history'] = []
        await update.message.reply_text("‚úÖ –î–∏–∞–ª–æ–≥ –æ—á–∏—â–µ–Ω.")
        return

    # –§–∏–ª—å—Ç—Ä –º—É—Å–æ—Ä–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    if len(user_message) < 2 or re.fullmatch(r"[!?.,\s]*", user_message):
        await update.message.reply_text("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –æ–±—É—á–µ–Ω–∏—é –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é.")
        return

    context.user_data.setdefault('chat_history', [])
    context.user_data['chat_history'].append({"role": "user", "content": user_message})

    retrieved_context = retrieve_context(user_message)
    if retrieved_context:
        system_msg = {"role": "system", "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {retrieved_context}"}
        messages = [system_msg] + context.user_data['chat_history'][-6:]
    else:
        messages = [{"role": "system", "content": TEACHER_ROLE}] + context.user_data['chat_history'][-6:]

    thinking = await update.message.reply_text("ü§î –î—É–º–∞—é...")
    loop = asyncio.get_running_loop()
    response = await loop.run_in_executor(None, lambda: ask_model_ollama(messages))
    await thinking.delete()
    
    context.user_data['chat_history'].append({"role": "assistant", "content": response})
    await update.message.reply_text(response)

async def handle_photo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    photo = update.message.photo[-1]
    file = await context.bot.get_file(photo.file_id)
    file_path = f"temp_{photo.file_id}.jpg"
    await file.download_to_drive(file_path)

    try:
        img_b64 = image_to_base64(file_path)
        user_msg = {"role": "user", "content": "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.", "images": [img_b64]}
        context.user_data.setdefault('chat_history', []).append(user_msg)

        messages = [{"role": "system", "content": TEACHER_ROLE}] + context.user_data['chat_history'][-6:]
        thinking = await update.message.reply_text("üñºÔ∏è –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")
        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(None, lambda: ask_model_ollama(messages))
        await thinking.delete()

        context.user_data['chat_history'].append({"role": "assistant", "content": response})
        await update.message.reply_text(response)

    finally:
        if os.path.exists(file_path):
            os.remove(file_path)

# === –ó–∞–ø—É—Å–∫ ===
def main():
    load_knowledge_base()
    application = Application.builder().token(BOT_TOKEN).build()
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text))
    application.add_handler(MessageHandler(filters.PHOTO, handle_photo))
    print("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ Ollama.")
    application.run_polling()

if __name__ == "__main__":
    main()