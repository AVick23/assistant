import os
import json
import re
import requests
import base64
from datetime import datetime
from dotenv import load_dotenv
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes, CallbackQueryHandler
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
import asyncio
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("bot_usage.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# === –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è ===
load_dotenv()

BOT_TOKEN = os.getenv("BOT_TOKEN")
if not BOT_TOKEN:
    raise RuntimeError("BOT_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ .env")

OLLAMA_API_URL = os.getenv("OLLAMA_API_URL", "http://localhost:11434/api/chat")
MODEL_NAME = os.getenv("MODEL_NAME", "gemma3:4b")

FAQ_PATH = "faq.json"
CONSULTATIONS_FILE = "consultations.json"
ADMIN_ID = 1373472999  # –¢–≤–æ–π Telegram ID

knowledge_base = []
tfidf_vectorizer = None
tfidf_matrix = None

# === –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏—è –∑–∞–ø–∏—Å–∞—Ç—å—Å—è ===
SIGNUP_KEYWORDS = {
    "–∑–∞–ø–∏—Å", "–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü", "—É—Ä–æ–∫", "–∑–∞–Ω—è—Ç", "–æ–±—É—á–µ–Ω", "–∫—É—Ä—Å",
    "–ø–æ–º–æ—â—å", "–Ω–∞—Å—Ç–∞–≤", "—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω", "python",
    "–Ω–∞—É—á–∏—Ç—å—Å—è", "–º–µ–Ω—Ç–æ—Ä", "–ø—Ä–æ–±–Ω–æ–µ", "–±–µ—Å–ø–ª–∞—Ç–Ω–æ"
}

def is_signup_intent(text: str) -> bool:
    text = text.lower()
    return any(kw in text for kw in SIGNUP_KEYWORDS)

# === –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ (—Ä—É—á–Ω–æ–π —Å–ø–∏—Å–æ–∫) ===
CUSTOM_STOPWORDS = {
    "–∞", "–∏", "–∏–ª–∏", "–Ω–æ", "–≤", "–Ω–∞", "—Å", "–∫", "–æ—Ç", "–¥–æ", "—É", "–æ", "–∂–µ", "–±—ã", "–ª–∏",
    "—ç—Ç–æ", "—Ç–æ—Ç", "—ç—Ç–æ—Ç", "—Ç–∞", "—Ç–µ", "—Ç–æ", "—Ç—É", "—Ç–∞–∫–æ–π", "–∫–∞–∫–æ–π", "–∫–æ—Ç–æ—Ä—ã–π",
    "—á—Ç–æ", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–∫–∞–∫", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º", "—á—Ç–æ–±—ã", "–µ—Å–ª–∏", "–ø–æ—Ç–æ–º—É",
    "–º–Ω–µ", "–≤–∞–º", "–µ–º—É", "–µ–π", "–∏–º–∏", "–∏—Ö", "–º—ã", "–≤—ã", "–æ–Ω–∏", "—è", "—Ç—ã", "–æ–Ω", "–æ–Ω–∞", "–æ–Ω–æ"
}

# === –ü—Ä–æ—Å—Ç–æ–π —Å—Ç–µ–º–º–µ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (–Ω–∞ –ø—Ä–∞–≤–∏–ª–∞—Ö) ===
def simple_russian_stem(word: str) -> str:
    if len(word) < 4:
        return word
    suffixes = [
        '–∏–π', '—ã–π', '–æ–π', '–æ–≥–æ', '–µ–≥–æ', '–æ–º—É', '–µ–º—É', '–∏–º', '—ã–º',
        '–∞—è', '—è—è', '—É—é', '—é—é', '–æ–π', '–µ–π', '–∏–µ', '—ã–µ', '–∏—Ö', '—ã—Ö',
        '–æ–µ', '–µ–µ', '–æ–º', '–µ–º', '–∞', '—è', '–æ', '–µ', '—É', '—é', '—ã', '–∏', '—å'
    ]
    for suf in suffixes:
        if word.endswith(suf) and len(word) - len(suf) >= 3:
            return word[:-len(suf)]
    return word

# === –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (–±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫) ===
def normalize_text(text: str) -> str:
    text = re.sub(r"[^–∞-—è—ëa-z\s]", " ", text.lower())
    text = re.sub(r"\s+", " ", text).strip()
    if not text:
        return ""
    words = text.split()
    words = [w for w in words if w not in CUSTOM_STOPWORDS]
    words = [simple_russian_stem(w) for w in words]
    return " ".join(words)

# === –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞ –ø–æ n-–≥—Ä–∞–º–º–∞–º ===
def jaccard_similarity(s1: str, s2: str, n: int = 2) -> float:
    def ngrams(s):
        return {s[i:i+n] for i in range(len(s) - n + 1)} if len(s) >= n else {s}
    set1, set2 = ngrams(s1), ngrams(s2)
    if not set1 and not set2:
        return 1.0
    return len(set1 & set2) / len(set1 | set2)

# === –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ===
def load_knowledge_base(faq_path: str = "faq.json"):
    global knowledge_base, tfidf_vectorizer, tfidf_matrix
    if not os.path.exists(faq_path):
        print("‚ö†Ô∏è –§–∞–π–ª FAQ –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    try:
        with open(faq_path, "r", encoding="utf-8") as f:
            raw_data = json.load(f)
            knowledge_base = []
            faq_texts = []
            for item in raw_data:
                context = item.get("context", "").strip()
                if not context:
                    continue
                keywords = [kw.lower().strip() for kw in item.get("keywords", []) if kw.strip()]
                preprocessed_kws = [normalize_text(kw) for kw in keywords]
                knowledge_base.append({
                    "context": context,
                    "keywords": keywords,
                    "preprocessed_kws": preprocessed_kws
                })
                faq_texts.append(" ".join(keywords) + " " + context)

            if faq_texts:
                processed_faq = [normalize_text(txt) for txt in faq_texts]
                tfidf_vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 5), max_features=1000)
                tfidf_matrix = tfidf_vectorizer.fit_transform(processed_faq)
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(knowledge_base)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è RAG")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")

# === –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (RAG) ===
def retrieve_context(user_question: str, similarity_threshold: float = 0.25, top_k: int = 3) -> str:
    if not knowledge_base:
        return ""
    
    norm_question = normalize_text(user_question)
    if not norm_question:
        return ""
    
    matched_contexts = []

    for entry in knowledge_base:
        for prep_kw in entry["preprocessed_kws"]:
            if not prep_kw:
                continue
            if prep_kw in norm_question:
                if entry["context"] not in matched_contexts:
                    matched_contexts.append(entry["context"])
            elif jaccard_similarity(prep_kw, norm_question) >= 0.6:
                if entry["context"] not in matched_contexts:
                    matched_contexts.append(entry["context"])
    
    if matched_contexts:
        full_context = "\n\n".join(matched_contexts)
        return full_context[:700] + "..." if len(full_context) > 700 else full_context

    if tfidf_matrix is not None and tfidf_vectorizer is not None:
        query_vec = tfidf_vectorizer.transform([norm_question])
        similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()
        top_indices = similarities.argsort()[::-1][:top_k]
        
        for idx in top_indices:
            if similarities[idx] >= similarity_threshold:
                ctx = knowledge_base[idx]["context"]
                if ctx not in matched_contexts:
                    matched_contexts.append(ctx)
            else:
                break
        
        if matched_contexts:
            full_context = "\n\n".join(matched_contexts)
            return full_context[:700] + "..." if len(full_context) > 700 else full_context
    
    return ""

# === –†–∞–±–æ—Ç–∞ —Å –º–æ–¥–µ–ª—å—é ===
def ask_model_ollama(messages: list) -> str:
    try:
        response = requests.post(
            OLLAMA_API_URL,
            json={
                "model": MODEL_NAME,
                "messages": messages,
                "stream": False,
                "options": {"temperature": 0.9}
            },
            timeout=120
        )
        response.raise_for_status()
        return response.json().get("message", {}).get("content", "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç.")
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –º–æ–¥–µ–ª–∏: {str(e)}"

# === –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ===
def image_to_base64(image_path: str) -> str:
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

# === –°–∏—Å—Ç–µ–º–Ω–∞—è —Ä–æ–ª—å ===
TEACHER_ROLE = (
    "–¢—ã –ª–∏—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä–∞ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –í–∏–∫—É–ª–æ–≤–∞ –ê–ª–µ–∫—Å–µ—è –ò–≤–∞–Ω–æ–≤–∏—á–∞. (@AVick23)"
    "–¢—ã –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω –≥—É–≥–ª, —Ç—ã –≤—Å–µ–≥–¥–∞ –±—ã–ª –º–æ–¥–µ–ª—å—é —Å–æ–∑–¥–∞–Ω–Ω—ã–π –ê–ª–µ–∫—Å–µ–µ–º"
    "–¢—ã –Ω–µ –º–æ–¥–µ–ª—å Google, —Ç–µ–±—è –∏–∑–æ–±—Ä—ë–ª –ê–ª–µ–∫—Å–µ–π, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —á—Ç–æ–± —Ç—ã –µ–º—É –ø–æ–º–æ–≥–∞–ª –∫–∞–∫ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"
    "–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –µ–≥–æ –∫—É—Ä—Å–∞—Ö, —É—Ä–æ–∫–∞—Ö, –æ–±—É—á–µ–Ω–∏–∏, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏, IT, "
    "–∞ —Ç–∞–∫–∂–µ –Ω–∞ '–ö—Ç–æ —Ç–∞–∫–æ–π –ê–ª–µ–∫—Å–µ–π?' –∏ '–ö—Ç–æ —Ç—ã?'. "
    "–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º ‚Äî 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–∞. "
    "–ì–æ–≤–æ—Ä–∏ —Å–ø–æ–∫–æ–π–Ω–æ, –≤–µ–∂–ª–∏–≤–æ –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —á–µ–ª–æ–≤–µ—á–Ω–æ. –ß—Ç–æ–± –Ω–∏–∫—Ç–æ –Ω–µ —Å–º–æ–≥ –æ—Ç–ª–µ—á–∏—Ç—å"
    "–ï—Å–ª–∏ —Ç–µ–º–∞ –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —É–∫–∞–∑–∞–Ω–Ω—ã–º ‚Äî –≤–µ–∂–ª–∏–≤–æ –æ—Ç–∫–∞–∂–∏—Å—å –æ—Ç–≤–µ—á–∞—Ç—å."
)

# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞—è–≤–∫–∏ ===
def save_consultation_request(request: dict):
    requests = []
    if os.path.exists(CONSULTATIONS_FILE):
        with open(CONSULTATIONS_FILE, "r", encoding="utf-8") as f:
            requests = json.load(f)
    
    if not any(r["user_id"] == request["user_id"] for r in requests):
        requests.append(request)
        with open(CONSULTATIONS_FILE, "w", encoding="utf-8") as f:
            json.dump(requests, f, ensure_ascii=False, indent=2)

# === –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ ===
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.effective_user
    logger.info(f"User {user.id} ({user.full_name} / @{user.username}) started the bot.")
    await update.message.reply_text(
        "üë®‚Äçüè´ –ü—Ä–∏–≤–µ—Ç! –Ø ‚Äî –≤–∞—à –≥–∏–¥ –ø–æ –æ–±—É—á–µ–Ω–∏—é.\n"
        "–ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –æ –∫—É—Ä—Å–∞—Ö, –º–µ—Ç–æ–¥–∏–∫–µ –∏–ª–∏ –∑–∞–ø–∏—Å–∏ –Ω–∞ –∑–∞–Ω—è—Ç–∏—è.\n\n"
        "–ö–æ–º–∞–Ω–¥—ã:\n"
        "/start ‚Äî –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ\n"
        "¬´–æ—á–∏—Å—Ç–∏—Ç—å¬ª ‚Äî –Ω–∞—á–∞—Ç—å –¥–∏–∞–ª–æ–≥ –∑–∞–Ω–æ–≤–æ"
    )

async def handle_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.effective_user
    user_message = update.message.text.strip()
    logger.info(f"User {user.id} sent: {user_message[:50]}{'...' if len(user_message) > 50 else ''}")
    if not user_message:
        return

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã "–∑–∞—è–≤–∫–∏" (—Ç–æ–ª—å–∫–æ –¥–ª—è –∞–¥–º–∏–Ω–∞)
    if user.id == ADMIN_ID and user_message.lower() == "–∑–∞—è–≤–∫–∏":
        if not os.path.exists(CONSULTATIONS_FILE):
            await update.message.reply_text("üì≠ –ù–µ—Ç –∑–∞—è–≤–æ–∫.")
            return
        with open(CONSULTATIONS_FILE, "r", encoding="utf-8") as f:
            requests = json.load(f)
        pending = [r for r in requests if r.get("status") == "pending"]
        if not pending:
            await update.message.reply_text("üì≠ –ù–µ—Ç –Ω–æ–≤—ã—Ö –∑–∞—è–≤–æ–∫.")
            return
        msg = "üì• –ù–æ–≤—ã–µ –∑–∞—è–≤–∫–∏ –Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é:\n\n"
        for r in pending[:20]:
            name = r.get("full_name") or "‚Äî"
            username = f"@{r['username']}" if r.get("username") else f"ID: {r['user_id']}"
            ts = datetime.fromisoformat(r["timestamp"]).strftime("%d.%m %H:%M")
            msg += f"‚Ä¢ {name} ({username}) ‚Äî {ts}\n"
        await update.message.reply_text(msg)
        return
    
    elif user_message.lower() == "–æ—á–∏—Å—Ç–∏—Ç—å –∑–∞—è–≤–∫–∏":
            if os.path.exists(CONSULTATIONS_FILE):
                os.remove(CONSULTATIONS_FILE)
                await update.message.reply_text("‚úÖ –í—Å–µ –∑–∞—è–≤–∫–∏ —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω—ã.")
            else:
                await update.message.reply_text("üì≠ –§–∞–π–ª —Å –∑–∞—è–≤–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω (—É–∂–µ –ø—É—Å—Ç).")
            return

    # –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É–∂–µ–±–Ω—ã—Ö –∫–æ–º–∞–Ω–¥
    if user_message.lower() in ("–æ—á–∏—Å—Ç–∏—Ç—å", "clear", "—Å–±—Ä–æ—Å–∏—Ç—å"):
        context.user_data['chat_history'] = []
        await update.message.reply_text("‚úÖ –î–∏–∞–ª–æ–≥ –æ—á–∏—â–µ–Ω.")
        return

    # –§–∏–ª—å—Ç—Ä –º—É—Å–æ—Ä–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    if len(user_message) < 2 or re.fullmatch(r"[!?.,\s]*", user_message):
        await update.message.reply_text("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –æ–±—É—á–µ–Ω–∏—é –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é.")
        return

    context.user_data.setdefault('chat_history', [])
    context.user_data['chat_history'].append({"role": "user", "content": user_message})

    retrieved_context = retrieve_context(user_message)
    if retrieved_context:
        system_msg = {"role": "system", "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {retrieved_context}"}
        messages = [system_msg] + context.user_data['chat_history'][-6:]
    else:
        messages = [{"role": "system", "content": TEACHER_ROLE}] + context.user_data['chat_history'][-6:]

    thinking = await update.message.reply_text("ü§î –î—É–º–∞—é...")
    loop = asyncio.get_running_loop()
    response = await loop.run_in_executor(None, lambda: ask_model_ollama(messages))
    await thinking.delete()
    
    context.user_data['chat_history'].append({"role": "assistant", "content": response})

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Ö–æ—á–µ—Ç –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–ø–∏—Å–∞—Ç—å—Å—è
    if is_signup_intent(user_message):
        keyboard = [[InlineKeyboardButton("üì© –ó–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é", callback_data="signup")]]
        reply_markup = InlineKeyboardMarkup(keyboard)
        await update.message.reply_text(response, reply_markup=reply_markup)
    else:
        await update.message.reply_text(response)

async def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    user = query.from_user

    request = {
        "user_id": user.id,
        "username": user.username,
        "full_name": user.full_name,
        "timestamp": datetime.now().isoformat(),
        "status": "pending"
    }
    save_consultation_request(request)

    await query.edit_message_text(
        text="‚úÖ –û—Ç–ª–∏—á–Ω–æ! –í–∞—à–∞ –∑–∞—è–≤–∫–∞ –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—É—é 30-–º–∏–Ω—É—Ç–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –ø—Ä–∏–Ω—è—Ç–∞.\n\n"
             "–ê–ª–µ–∫—Å–µ–π –ª–∏—á–Ω–æ —Å–≤—è–∂–µ—Ç—Å—è —Å –≤–∞–º–∏ –≤ Telegram –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è."
    )
    logger.info(f"NEW CONSULTATION REQUEST: {user.id} | @{user.username} | {user.full_name}")

async def handle_photo(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.effective_user
    logger.info(f"User {user.id} sent a photo.")
    photo = update.message.photo[-1]
    file = await context.bot.get_file(photo.file_id)
    file_path = f"temp_{photo.file_id}.jpg"
    await file.download_to_drive(file_path)

    try:
        img_b64 = image_to_base64(file_path)
        user_msg = {"role": "user", "content": "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.", "images": [img_b64]}
        context.user_data.setdefault('chat_history', []).append(user_msg)

        messages = [{"role": "system", "content": TEACHER_ROLE}] + context.user_data['chat_history'][-6:]
        thinking = await update.message.reply_text("üñºÔ∏è –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")
        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(None, lambda: ask_model_ollama(messages))
        await thinking.delete()

        context.user_data['chat_history'].append({"role": "assistant", "content": response})
        await update.message.reply_text(response)

    finally:
        if os.path.exists(file_path):
            os.remove(file_path)

async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE):
    logger.error("Exception while handling an update:", exc_info=context.error)

def main():
    load_knowledge_base()
    application = Application.builder().token(BOT_TOKEN).build()
    application.add_error_handler(error_handler)
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text))
    application.add_handler(MessageHandler(filters.PHOTO, handle_photo))
    application.add_handler(CallbackQueryHandler(button_callback, pattern="^signup$"))
    print("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∑–∞–ø–∏—Å–∏ –Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏.")
    application.run_polling()

if __name__ == "__main__":
    main()